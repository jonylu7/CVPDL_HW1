{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ae9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally, using local path: /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1\n",
      "DATA_YAML: /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9_data.yaml\n",
      "WEIGHTS: /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/weights/yolov9-s.pt\n",
      "PROJECT: /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/runs/train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import csv, re, shutil, subprocess, sys\n",
    "from typing import List, Tuple\n",
    "# ÂÅµÊ∏¨ÊòØÂê¶Âú® Colab\n",
    "def in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if in_colab():\n",
    "    # Â¶ÇÊûúÂú® ColabÔºåÊéõËºâ Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    drive_root = Path(\"/content/drive/MyDrive\")\n",
    "    PROJECT_DIR = drive_root / \"CVPDL_HW1\"\n",
    "    print(\"Running on Colab, using Google Drive path:\", PROJECT_DIR)\n",
    "\n",
    "    #update this repo\n",
    "    #subprocess.run([\"git\", \"pull\"], cwd=PROJECT_DIR)\n",
    "\n",
    "\n",
    "else:\n",
    "    # Â¶ÇÊûúÂú®Êú¨Âú∞Á´Ø\n",
    "    PROJECT_DIR = Path(os.getcwd())\n",
    "    print(\"Running locally, using local path:\", PROJECT_DIR)\n",
    "\n",
    "# ÁØÑ‰æãÔºöËá™ÂãïË®≠ÂÆöÂ∏∏Áî®Ê™îÊ°àË∑ØÂæë\n",
    "DATA_YAML = PROJECT_DIR / \"yolov9_data.yaml\"\n",
    "WEIGHTS   = PROJECT_DIR / \"weights\" / \"yolov9-s.pt\"\n",
    "PROJECT   = PROJECT_DIR / \"runs\" / \"train\"\n",
    "\n",
    "print(\"DATA_YAML:\", DATA_YAML)\n",
    "print(\"WEIGHTS:\", WEIGHTS)\n",
    "print(\"PROJECT:\", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a94bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone yolov9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2700906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 2Ô∏è‚É£ Ë®≠ÂÆöÂ≠êË≥áÊñôÂ§æ ==========\n",
    "DATASET = PROJECT_DIR / \"taica-cvpdl-2025-hw-1\"\n",
    "TRAIN_IMG_DIR = DATASET / \"train\" / \"img\"\n",
    "TEST_IMG_DIR  = DATASET / \"test\" / \"img\"\n",
    "GT_FILE = DATASET / \"train\" / \"gt.txt\"\n",
    "\n",
    "LABELS_DIR = DATASET / \"train\" / \"labels\"\n",
    "YAML_PATH  = PROJECT_DIR / \"yolov9_data.yaml\"\n",
    "REPO_DIR   = PROJECT_DIR / \"yolov9\"\n",
    "WEIGHTS_DIR = PROJECT_DIR / \"weights\"\n",
    "SUBMISSION_DIR = PROJECT_DIR / \"submission\"\n",
    "CFG_DIR=PROJECT_DIR / \"cfg\"\n",
    "\n",
    "for d in [LABELS_DIR, WEIGHTS_DIR, SUBMISSION_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499e91ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gitpython in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.1.45)\n",
      "Requirement already satisfied: ipython in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (8.37.0)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.24.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.6.0.66)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (9.1.1)\n",
      "Requirement already satisfied: psutil in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (7.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.27.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (1.15.3)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.13.0.dev20220608)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.14.0a0+f9f721d)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (4.64.0)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (2.20.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (2.3.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: albumentations>=1.0.3 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (2.0.8)\n",
      "Requirement already satisfied: pycocotools>=2.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (2.0.10)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from gitpython->-r requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: exceptiongroup in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: decorator in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (5.14.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (2.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (3.0.52)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: stack_data in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 7)) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 7)) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 13)) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 13)) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 13)) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 13)) (2.0.12)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (2.3.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (0.7.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (1.75.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (62.3.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 22)) (4.21.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from albumentations>=1.0.3->-r requirements.txt (line 46)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from albumentations>=1.0.3->-r requirements.txt (line 46)) (4.11.0.86)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from albumentations>=1.0.3->-r requirements.txt (line 46)) (2.11.10)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from albucore==0.0.24->albumentations>=1.0.3->-r requirements.txt (line 46)) (4.1.0)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from albucore==0.0.24->albumentations>=1.0.3->-r requirements.txt (line 46)) (6.5.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->-r requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 6)) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from pexpect>4.3->ipython->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->-r requirements.txt (line 6)) (0.2.14)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations>=1.0.3->-r requirements.txt (line 46)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations>=1.0.3->-r requirements.txt (line 46)) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations>=1.0.3->-r requirements.txt (line 46)) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 22)) (3.0.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from stack_data->ipython->-r requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from stack_data->ipython->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/luweiren/miniforge3/envs/PytorchLearning/lib/python3.10/site-packages (from stack_data->ipython->-r requirements.txt (line 6)) (0.2.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/Users/luweiren/miniforge3/envs/PytorchLearning/bin/python', '-m', 'pip', 'install', '-r', 'requirements.txt'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not REPO_DIR.exists():\n",
    "    print(\"üì¶ Cloning YOLOv9 repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/WongKinYiu/yolov9.git\", str(REPO_DIR)], check=True)\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"], cwd=REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e63125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Box to YOLO format conversion example:\n",
      "Input: left=100, top=50, width=200, height=150 (in 640x480 image)\n",
      "Output: 0 0.312500 0.260417 0.312500 0.312500\n",
      "Explanation: class=0, center_x=(100+200/2)/640=0.3125, center_y=(50+150/2)/480=0.2604, w=200/640=0.3125, h=150/480=0.3125\n"
     ]
    }
   ],
   "source": [
    "# ========== 4Ô∏è‚É£ Âü∫Êú¨Â∑•ÂÖ∑ÂáΩÂºè ==========\n",
    "def list_images(img_dir: Path) -> List[Path]:\n",
    "    return sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\")))\n",
    "\n",
    "def im_size(path: Path) -> Tuple[int, int]:\n",
    "    with Image.open(path) as im:\n",
    "        return im.size\n",
    "\n",
    "def to_int_id(name_or_id: str) -> int:\n",
    "    s = re.sub(r\"^0+\", \"\", name_or_id.replace(\".jpg\",\"\").replace(\".png\",\"\"))\n",
    "    return int(s) if s else 0\n",
    "\n",
    "def parse_gt_line(line: str):\n",
    "    \"\"\"\n",
    "    Ëß£Êûê gt.txt ÁöÑ‰∏ÄË°å\n",
    "    Ê†ºÂºè: image_id,left,top,width,height (ÂèØËÉΩÊúâÂ§öÂÄã box)\n",
    "    ËøîÂõû: (image_id, [(left, top, width, height), ...])\n",
    "    \"\"\"\n",
    "    toks = re.split(r\"[,\\s]+\", line.strip())\n",
    "    if len(toks) < 5: \n",
    "        return \"\", []\n",
    "    head = toks[0]  # image_id\n",
    "    rest = toks[1:]  # Ââ©‰∏ãÁöÑÊòØÂùêÊ®ô\n",
    "    boxes = []\n",
    "    for i in range(0, len(rest), 4):  # ÊØè 4 ÂÄã‰∏ÄÁµÑ (left, top, width, height)\n",
    "        if i + 3 < len(rest):\n",
    "            try:\n",
    "                left = float(rest[i])\n",
    "                top = float(rest[i+1])\n",
    "                width = float(rest[i+2])\n",
    "                height = float(rest[i+3])\n",
    "                boxes.append((left, top, width, height))\n",
    "            except:\n",
    "                pass\n",
    "    return head, boxes\n",
    "\n",
    "def box_to_yolo_format(left: float, top: float, width: float, height: float, \n",
    "                       img_width: int, img_height: int, class_id: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Â∞á bounding box Âæû (left, top, width, height) ÂÉèÁ¥†Ê†ºÂºè\n",
    "    ËΩâÊèõÁÇ∫ YOLO Ê†ºÂºè: class center_x center_y width height (Ê≠∏‰∏ÄÂåñ)\n",
    "    \n",
    "    ÂèÉÊï∏:\n",
    "        left: Â∑¶‰∏äËßí x ÂùêÊ®ô (ÂÉèÁ¥†)\n",
    "        top: Â∑¶‰∏äËßí y ÂùêÊ®ô (ÂÉèÁ¥†)\n",
    "        width: ÂØ¨Â∫¶ (ÂÉèÁ¥†)\n",
    "        height: È´òÂ∫¶ (ÂÉèÁ¥†)\n",
    "        img_width: ÂúñÁâáÂØ¨Â∫¶\n",
    "        img_height: ÂúñÁâáÈ´òÂ∫¶\n",
    "        class_id: È°ûÂà• ID (È†êË®≠ 0)\n",
    "    \n",
    "    ËøîÂõû:\n",
    "        YOLO Ê†ºÂºèÂ≠ó‰∏≤: \"class cx cy w h\"\n",
    "    \"\"\"\n",
    "    # Ë®àÁÆó‰∏≠ÂøÉÈªûÂùêÊ®ô\n",
    "    center_x = (left + width / 2.0) / img_width\n",
    "    center_y = (top + height / 2.0) / img_height\n",
    "    \n",
    "    # Ê≠∏‰∏ÄÂåñÂØ¨È´ò\n",
    "    norm_width = width / img_width\n",
    "    norm_height = height / img_height\n",
    "    \n",
    "    # ÈôêÂà∂Âú® [0, 1] ÁØÑÂúçÂÖß\n",
    "    center_x = max(0.0, min(1.0, center_x))\n",
    "    center_y = max(0.0, min(1.0, center_y))\n",
    "    norm_width = max(0.0, min(1.0, norm_width))\n",
    "    norm_height = max(0.0, min(1.0, norm_height))\n",
    "    \n",
    "    return f\"{class_id} {center_x:.6f} {center_y:.6f} {norm_width:.6f} {norm_height:.6f}\"\n",
    "\n",
    "# Ê∏¨Ë©¶ËΩâÊèõÂáΩÊï∏\n",
    "print(\"üìù Box to YOLO format conversion example:\")\n",
    "print(\"Input: left=100, top=50, width=200, height=150 (in 640x480 image)\")\n",
    "result = box_to_yolo_format(100, 50, 200, 150, 640, 480, 0)\n",
    "print(f\"Output: {result}\")\n",
    "print(\"Explanation: class=0, center_x=(100+200/2)/640=0.3125, center_y=(50+150/2)/480=0.2604, w=200/640=0.3125, h=150/480=0.3125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd16051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ YOLO labels saved at: /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/taica-cvpdl-2025-hw-1/train/labels\n",
      "üìä Converted: 38619 images\n",
      "‚ö†Ô∏è  Skipped: 128 images (not found)\n"
     ]
    }
   ],
   "source": [
    "# ========== 5Ô∏è‚É£ Â∞á gt.txt ËΩâÊàê YOLO Ê®ôÁ±§ ==========\n",
    "def convert_gt_to_yolo_labels():\n",
    "    \"\"\"\n",
    "    Â∞á gt.txt ‰∏≠ÁöÑ bounding boxes ËΩâÊèõÁÇ∫ YOLO Ê†ºÂºè\n",
    "    gt.txt Ê†ºÂºè: image_id,left,top,width,height\n",
    "    YOLO Ê†ºÂºè: class center_x center_y width height (Ê≠∏‰∏ÄÂåñÂà∞ 0-1)\n",
    "    \"\"\"\n",
    "    imgs = {p.name: p for p in list_images(TRAIN_IMG_DIR)}\n",
    "    converted_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    with open(GT_FILE, \"r\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            name_or_id, boxes = parse_gt_line(line)\n",
    "            if not boxes: \n",
    "                continue\n",
    "            \n",
    "            # Â∞á image_id ËΩâÊèõÁÇ∫Ê™îÂêçÊ†ºÂºè\n",
    "            key = f\"{to_int_id(name_or_id):08d}.jpg\"\n",
    "            img_path = imgs.get(key)\n",
    "            \n",
    "            if(img_path is None):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            # Áç≤ÂèñÂúñÁâáÂ∞∫ÂØ∏\n",
    "            img_width, img_height = im_size(img_path)\n",
    "            \n",
    "            \n",
    "            # ËΩâÊèõÊØèÂÄã box Âà∞ YOLO Ê†ºÂºè\n",
    "            yolo_lines = []\n",
    "            for left, top, width, height in boxes:\n",
    "                yolo_line = box_to_yolo_format(left, top, width, height, \n",
    "                                               img_width, img_height, \n",
    "                                               class_id=0)  # ÂñÆÈ°ûÂà•Ôºåclass_id=0\n",
    "                yolo_lines.append(yolo_line)\n",
    "            \n",
    "            # ÂØ´ÂÖ• YOLO Ê®ôÁ±§Ê™îÊ°à\n",
    "            label_file = LABELS_DIR / f\"{Path(key).stem}.txt\"\n",
    "            if(label_file.exists()):\n",
    "                with open(label_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(yolo_lines) + \"\\n\")\n",
    "            else:\n",
    "                with open(label_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\\n\".join(yolo_lines) + \"\\n\")\n",
    "            converted_count += 1\n",
    "    \n",
    "    print(f\"‚úÖ YOLO labels saved at: {LABELS_DIR}\")\n",
    "    print(f\"üìä Converted: {converted_count} images\")\n",
    "    print(f\"‚ö†Ô∏è  Skipped: {skipped_count} images (not found)\")\n",
    "\n",
    "#clear labels\n",
    "for label_file in LABELS_DIR.glob(\"*.txt\"):\n",
    "    label_file.unlink()\n",
    "\n",
    "convert_gt_to_yolo_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8585e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wrote YAML to /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9_data.yaml\n"
     ]
    }
   ],
   "source": [
    "# ========== 6Ô∏è‚É£ ÂØ´ÂÖ• data.yaml ==========\n",
    "YAML_PATH.write_text(f\"\"\"# YOLOv9 data file\n",
    "train: {TRAIN_IMG_DIR.as_posix()}\n",
    "val: {TRAIN_IMG_DIR.as_posix()}\n",
    "test: {TEST_IMG_DIR.as_posix()}\n",
    "nc: 1\n",
    "names: ['object']\n",
    "\"\"\")\n",
    "print(f\"‚úÖ Wrote YAML to {YAML_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72eaaf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/weights/yolov9-s.pt, cfg=/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/cfg/yolov9-s.yaml, data=/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9_data.yaml, hyp=/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/data/hyps/hyp.scratch-high.yaml, epochs=50, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=cvpdl_hw1, exist_ok=True, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=0, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "YOLO üöÄ v0.1-104-g5b1ea9a Python-3.10.4 torch-1.13.0.dev20220608 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO üöÄ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 2]                 \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     31104  models.common.ELAN1                     [64, 64, 64, 32]              \n",
      "  3                -1  1     73984  models.common.AConv                     [64, 128]                     \n",
      "  4                -1  1    258432  models.common.RepNCSPELAN4              [128, 128, 128, 64, 3]        \n",
      "  5                -1  1    221568  models.common.AConv                     [128, 192]                    \n",
      "  6                -1  1    579648  models.common.RepNCSPELAN4              [192, 192, 192, 96, 3]        \n",
      "  7                -1  1    442880  models.common.AConv                     [192, 256]                    \n",
      "  8                -1  1   1028864  models.common.RepNCSPELAN4              [256, 256, 256, 128, 3]       \n",
      "  9                -1  1    164608  models.common.SPPELAN                   [256, 256, 128]               \n",
      " 10                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 11           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 12                -1  1    628800  models.common.RepNCSPELAN4              [448, 192, 192, 96, 3]        \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    283008  models.common.RepNCSPELAN4              [320, 128, 128, 64, 3]        \n",
      " 16                -1  1    110784  models.common.AConv                     [128, 96]                     \n",
      " 17          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 18                -1  1    598080  models.common.RepNCSPELAN4              [288, 192, 192, 96, 3]        \n",
      " 19                -1  1    221440  models.common.AConv                     [192, 128]                    \n",
      " 20           [-1, 9]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1   1061632  models.common.RepNCSPELAN4              [384, 256, 256, 128, 3]       \n",
      " 22                 8  1    164608  models.common.SPPELAN                   [256, 256, 128]               \n",
      " 23                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 24           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 25                -1  1    628800  models.common.RepNCSPELAN4              [448, 192, 192, 96, 3]        \n",
      " 26                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 27           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 28                -1  1    283008  models.common.RepNCSPELAN4              [320, 128, 128, 64, 3]        \n",
      " 29[28, 25, 22, 15, 18, 21]  1   2942630  models.yolo.DualDDetect                 [1, [128, 192, 256, 128, 192, 256]]\n",
      "yolov9-s summary: 1219 layers, 9743366 parameters, 9743334 gradients, 39.6 GFLOPs\n",
      "\n",
      "Transferred 1760/1772 items from /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/weights/yolov9-s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 291 weight(decay=0.0), 305 weight(decay=0.0005), 303 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0m1 validation error for InitSchema\n",
      "size\n",
      "  Field required [type=missing, input_value={'scale': (0.8, 1.0), 'ra...: None, 'strict': False}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/taica-cvpdl-2025-hw-1/train/labels... 1266 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1266/1266 00:04\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/taica-cvpdl-2025-hw-1/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/taica-cvpdl-2025-hw-1/train/labels.cache... 1266 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1266/1266 00:00\n",
      "Plotting labels to runs/train/cvpdl_hw1/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/cvpdl_hw1\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/159 00:04\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/train.py\", line 634, in <module>\n",
      "    main(opt)\n",
      "  File \"/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/train.py\", line 528, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/train.py\", line 304, in train\n",
      "    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
      "  File \"/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/utils/loss_tal.py\", line 168, in __call__\n",
      "    pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
      "  File \"/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/utils/loss_tal.py\", line 168, in <listcomp>\n",
      "    pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
      "AttributeError: 'list' object has no attribute 'view'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/Users/luweiren/miniforge3/envs/PytorchLearning/bin/python', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/train.py', '--data', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9_data.yaml', '--weights', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/weights/yolov9-s.pt', '--img', '640', '--epochs', '50', '--batch', '8', '--device', 'cpu', '--project', 'runs/train', '--name', 'cvpdl_hw1', '--cfg', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/cfg/yolov9-s.yaml', '--hyp', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/data/hyps/hyp.scratch-high.yaml', '--exist-ok']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ÈñãÂßãË®ìÁ∑¥\u001b[39;00m\n\u001b[1;32m     11\u001b[0m train_cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;28mstr\u001b[39m(REPO_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.py\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(YAML_PATH),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--exist-ok\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m ]\n\u001b[0;32m---> 25\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# ========== 8Ô∏è‚É£ Â∞ç test ÈõÜÊé®Ë´ñ ==========\u001b[39;00m\n\u001b[1;32m     28\u001b[0m best_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m((REPO_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/PytorchLearning/lib/python3.10/subprocess.py:524\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    525\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/Users/luweiren/miniforge3/envs/PytorchLearning/bin/python', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/train.py', '--data', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9_data.yaml', '--weights', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/weights/yolov9-s.pt', '--img', '640', '--epochs', '50', '--batch', '8', '--device', 'cpu', '--project', 'runs/train', '--name', 'cvpdl_hw1', '--cfg', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/cfg/yolov9-s.yaml', '--hyp', '/Users/luweiren/Documents/Projects/CVPDL/CVPDL_HW1/yolov9/data/hyps/hyp.scratch-high.yaml', '--exist-ok']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# ========== 7Ô∏è‚É£ Ë®ìÁ∑¥Ë®≠ÂÆö ==========\n",
    "# ÈÅ∏ÊìáÊ¨äÈáçÔºàÂª∫Ë≠∞ yolov9-c.pt Êàñ yolov9-e.ptÔºâ\n",
    "#!wget -nc https://github.com/WongKinYiu/yolov9/releases/download/v1.0/yolov9-s.pt -P {WEIGHTS_DIR}\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH  = 8\n",
    "IMGSZ  = 640\n",
    "DEVICE = \"cpu\" if not in_colab() else 0   # GPU\n",
    "\n",
    "# ÈñãÂßãË®ìÁ∑¥\n",
    "train_cmd = [\n",
    "    sys.executable, str(REPO_DIR / \"train.py\"),\n",
    "    \"--data\", str(YAML_PATH),\n",
    "    \"--weights\", str(WEIGHTS_DIR / \"yolov9-s.pt\"),\n",
    "    \"--img\", str(IMGSZ),\n",
    "    \"--epochs\", str(EPOCHS),\n",
    "    \"--batch\", str(BATCH),\n",
    "    \"--device\", str(DEVICE),\n",
    "    \"--project\", \"runs/train\",\n",
    "    \"--name\", \"cvpdl_hw1\",\n",
    "    \"--cfg\", str(CFG_DIR / \"yolov9-s.yaml\"),\n",
    "    \"--hyp\", str(REPO_DIR / \"data/hyps/hyp.scratch-high.yaml\"),\n",
    "    \"--exist-ok\"\n",
    "]\n",
    "subprocess.run(train_cmd,cwd=str(PROJECT_DIR), check=True)\n",
    "\n",
    "# ========== 8Ô∏è‚É£ Â∞ç test ÈõÜÊé®Ë´ñ ==========\n",
    "best_weight = sorted((REPO_DIR / \"runs/train\").rglob(\"best.pt\"))[-1]\n",
    "detect_cmd = [\n",
    "    \"python\", \"detect.py\",\n",
    "    \"--weights\", str(best_weight),\n",
    "    \"--source\", str(TEST_IMG_DIR),\n",
    "    \"--img\", str(IMGSZ),\n",
    "    \"--conf\", \"0.001\",\n",
    "    \"--iou\", \"0.6\",\n",
    "    \"--save-txt\", \"--save-conf\",\n",
    "    \"--project\", \"runs/detect\",\n",
    "    \"--name\", \"cvpdl_hw1_test\",\n",
    "    \"--exist-ok\"\n",
    "]\n",
    "subprocess.run(detect_cmd, cwd=REPO_DIR, check=True)\n",
    "print(\"‚úÖ Detection finished.\")\n",
    "\n",
    "# ========== 9Ô∏è‚É£ Áî¢Áîü Submission CSV ==========\n",
    "labels_dir = sorted((REPO_DIR / \"runs/detect\").glob(\"cvpdl_hw1_test*\"))[-1] / \"labels\"\n",
    "out_csv = SUBMISSION_DIR / f\"submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "def normalized_to_pixels(cx, cy, w, h, W, H):\n",
    "    return (cx - w/2)*W, (cy - h/2)*H, w*W, h*H\n",
    "\n",
    "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Image_ID\", \"PredictionString\"])\n",
    "    for img_path in list_images(TEST_IMG_DIR):\n",
    "        W,H = im_size(img_path)\n",
    "        stem = img_path.stem\n",
    "        img_id = to_int_id(stem)\n",
    "        lbl_path = labels_dir / f\"{stem}.txt\"\n",
    "        pred_str = \"\"\n",
    "        if lbl_path.exists():\n",
    "            lines = lbl_path.read_text().strip().splitlines()\n",
    "            preds = []\n",
    "            for line in lines:\n",
    "                c,conf,cx,cy,w,h = map(float,line.strip().split())\n",
    "                preds.append((c,conf,cx,cy,w,h))\n",
    "            preds.sort(key=lambda t:t[1], reverse=True)\n",
    "            parts=[]\n",
    "            for c,conf,cx,cy,w,h in preds:\n",
    "                x,y,ww,hh = normalized_to_pixels(cx,cy,w,h,W,H)\n",
    "                parts += [f\"{conf:.6f}\",f\"{x:.2f}\",f\"{y:.2f}\",f\"{ww:.2f}\",f\"{hh:.2f}\",f\"{int(c)}\"]\n",
    "            pred_str=\" \".join(parts)\n",
    "        writer.writerow([img_id,pred_str])\n",
    "\n",
    "print(f\"‚úÖ Submission file saved: {out_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
